{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN5GOEC2+wBbYcwR5vNFatu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Table of Contents\n","- [1 - Packages](#1)\n","- [2 - Util Functions](#2)\n","- [3 - Initialization](#3)\n","    - [3.1 - 2-layer Neural Network](#3-1)\n","    - [3.2 - L-layer Neural Network](#3-2)\n","- [4 - Forward Propagation Module](#4)\n","    - [4.1 - Linear Forward](#4-1)\n","    - [4.2 - Linear-Activation Forward](#4-2)\n","    - [4.3 - L-Layer Model](#4-3)\n","- [5 - Cost Function](#5)\n","- [6 - Backward Propagation Module](#6)\n","    - [6.1 - Linear Backward](#6-1)\n","    - [6.2 - Linear-Activation Backward](#6-2)\n","    - [6.3 - L-Model Backward](#6-3)\n","    - [6.4 - Update Parameters](#6-4)"],"metadata":{"id":"t1k2Vna_Gxky"}},{"cell_type":"markdown","source":["<a name='1'></a>\n","## 1 - Packages"],"metadata":{"id":"TfUe_hS5H-QV"}},{"cell_type":"code","source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","\n","import copy\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"],"metadata":{"id":"zljRYVKSDHxA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name='2'></a>\n","## 2 - Util Functions"],"metadata":{"id":"15EMkm2sIM8s"}},{"cell_type":"code","source":["import numpy as np\n","\n","def sigmoid(Z):\n","    \"\"\"\n","    Implements the sigmoid activation in numpy\n","\n","    Arguments:\n","    Z -- numpy array of any shape\n","\n","    Returns:\n","    A -- output of sigmoid(z), same shape as Z\n","    cache -- returns Z as well, useful during backpropagation\n","    \"\"\"\n","\n","    A = 1/(1+np.exp(-Z))\n","    cache = Z\n","\n","    return A, cache\n","\n","def relu(Z):\n","    \"\"\"\n","    Implement the RELU function.\n","\n","    Arguments:\n","    Z -- Output of the linear layer, of any shape\n","\n","    Returns:\n","    A -- Post-activation parameter, of the same shape as Z\n","    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","\n","    A = np.maximum(0,Z)\n","\n","    assert(A.shape == Z.shape)\n","\n","    cache = Z\n","    return A, cache\n","\n","\n","def relu_backward(dA, cache):\n","    \"\"\"\n","    Implement the backward propagation for a single RELU unit.\n","\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","    \"\"\"\n","\n","    Z = cache\n","    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n","\n","    # When z <= 0, you should set dz to 0 as well.\n","    dZ[Z <= 0] = 0\n","\n","    assert (dZ.shape == Z.shape)\n","\n","    return dZ\n","\n","def sigmoid_backward(dA, cache):\n","    \"\"\"\n","    Implement the backward propagation for a single SIGMOID unit.\n","\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","    \"\"\"\n","\n","    Z = cache\n","\n","    s = 1/(1+np.exp(-Z))\n","    dZ = dA * s * (1-s)\n","\n","    assert (dZ.shape == Z.shape)\n","\n","    return dZ\n","\n"],"metadata":{"id":"cJYW2aJGE-y2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name='ex-2'></a>\n","### Exercise 2 -  initialize_parameters_deep"],"metadata":{"id":"TTlu-EVcHIH1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"llcaMmQKCbRy"},"outputs":[],"source":["# GRADED FUNCTION: initialize_parameters_deep\n","\n","def initialize_parameters_deep(layer_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","\n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    bl -- bias vector of shape (layer_dims[l], 1)\n","    \"\"\"\n","\n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims) # number of layers in the network\n","\n","    for l in range(1, L):\n","\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","\n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","\n","    return parameters"]},{"cell_type":"code","source":["print(\"Test Case 1:\\n\")\n","parameters = initialize_parameters_deep([5,4,3])\n","\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))\n","\n","print(\"Test Case 2:\\n\")\n","parameters = initialize_parameters_deep([4,3,2])\n","\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkoq26NhCte6","executionInfo":{"status":"ok","timestamp":1719799763398,"user_tz":-480,"elapsed":2,"user":{"displayName":"王震","userId":"15815926928603043085"}},"outputId":"e702e07e-59a8-4efc-ce9f-1b11533d42f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Case 1:\n","\n","W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n"," [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n"," [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n"," [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n","b1 = [[0.]\n"," [0.]\n"," [0.]\n"," [0.]]\n","W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n"," [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n"," [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n","b2 = [[0.]\n"," [0.]\n"," [0.]]\n","Test Case 2:\n","\n","W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493]\n"," [-0.00277388 -0.00354759 -0.00082741 -0.00627001]\n"," [-0.00043818 -0.00477218 -0.01313865  0.00884622]]\n","b1 = [[0.]\n"," [0.]\n"," [0.]]\n","W2 = [[ 0.00881318  0.01709573  0.00050034]\n"," [-0.00404677 -0.0054536  -0.01546477]]\n","b2 = [[0.]\n"," [0.]]\n"]}]},{"cell_type":"markdown","source":["***Expected output***\n","```\n","Test Case 1:\n","\n","W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n"," [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n"," [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n"," [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n","b1 = [[0.]\n"," [0.]\n"," [0.]\n"," [0.]]\n","W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n"," [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n"," [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n","b2 = [[0.]\n"," [0.]\n"," [0.]]\n","\n","Test Case 2:\n","\n","W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493]\n"," [-0.00277388 -0.00354759 -0.00082741 -0.00627001]\n"," [-0.00043818 -0.00477218 -0.01313865  0.00884622]]\n","b1 = [[0.]\n"," [0.]\n"," [0.]]\n","W2 = [[ 0.00881318  0.01709573  0.00050034]\n"," [-0.00404677 -0.0054536  -0.01546477]]\n","b2 = [[0.]\n"," [0.]]\n","```"],"metadata":{"id":"AIyrU9TvDnuZ"}},{"cell_type":"markdown","source":["<a name='4'></a>\n","## 4 - Forward Propagation Module\n","\n","<a name='4-1'></a>\n","### 4.1 - Linear Forward"],"metadata":{"id":"WzEbpnkvHPcl"}},{"cell_type":"code","source":["# GRADED FUNCTION: linear_forward\n","\n","def linear_forward(A, W, b):\n","    \"\"\"\n","    Implement the linear part of a layer's forward propagation.\n","\n","    Arguments:\n","    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","\n","    Returns:\n","    Z -- the input of the activation function, also called pre-activation parameter\n","    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","\n","    Z = np.dot(W, A) + b\n","\n","    cache = (A, W, b)\n","\n","    return Z, cache"],"metadata":{"id":"cfwrJls5EC0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(1)\n","\n","t_A = np.random.randn(3,2)\n","t_W = np.random.randn(1,3)\n","t_b = np.random.randn(1,1)\n","\n","t_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n","print(\"Z = \" + str(t_Z))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j67NAEryEDso","executionInfo":{"status":"ok","timestamp":1719800008575,"user_tz":-480,"elapsed":360,"user":{"displayName":"王震","userId":"15815926928603043085"}},"outputId":"6478e6b1-5dd0-4ff6-fe11-33e985eb4f93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Z = [[ 3.26295337 -1.23429987]]\n"]}]},{"cell_type":"markdown","source":["<a name='4-2'></a>\n","### 4.2 - Linear-Activation Forward"],"metadata":{"id":"TeHT3UUSHSrg"}},{"cell_type":"code","source":["# GRADED FUNCTION: linear_activation_forward\n","\n","def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Implement the forward propagation for the LINEAR->ACTIVATION layer\n","\n","    Arguments:\n","    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    A -- the output of the activation function, also called the post-activation value\n","    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n","             stored for computing the backward pass efficiently\n","    \"\"\"\n","\n","    if activation == \"sigmoid\":\n","\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z)\n","\n","    elif activation == \"relu\":\n","\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"],"metadata":{"id":"mFlzdFzoEb2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(2)\n","\n","t_A_prev = np.random.randn(3,2)\n","t_W = np.random.randn(1,3)\n","t_b = np.random.randn(1,1)\n","\n","t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\")\n","print(\"With sigmoid: A = \" + str(t_A))\n","\n","t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"relu\")\n","print(\"With ReLU: A = \" + str(t_A))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6k9MTFBcEdfh","executionInfo":{"status":"ok","timestamp":1719800174461,"user_tz":-480,"elapsed":336,"user":{"displayName":"王震","userId":"15815926928603043085"}},"outputId":"0f64f02b-7689-49c1-d26c-ff96950a56bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["With sigmoid: A = [[0.96890023 0.11013289]]\n","With ReLU: A = [[3.43896131 0.        ]]\n"]}]},{"cell_type":"markdown","source":["<a name='4-3'></a>\n","### 4.3 - L-Layer Model"],"metadata":{"id":"LpQbU2A2HeY5"}},{"cell_type":"code","source":["# GRADED FUNCTION: L_model_forward\n","\n","def L_model_forward(X, parameters):\n","    \"\"\"\n","    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n","\n","    Arguments:\n","    X -- data, numpy array of shape (input size, number of examples)\n","    parameters -- output of initialize_parameters_deep()\n","\n","    Returns:\n","    AL -- activation value from the output (last) layer\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n","    \"\"\"\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2                  # number of layers in the neural network\n","\n","    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n","    # The for loop starts at 1 because layer 0 is the input\n","    for l in range(1, L):\n","        A_prev = A\n","\n","        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation = \"relu\")\n","        caches.append(cache)\n","\n","    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n","\n","    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = \"sigmoid\")\n","    caches.append(cache)\n","\n","    return AL, caches"],"metadata":{"id":"DYAr2TnoDK2L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(6)\n","\n","t_X = np.random.randn(5,4)\n","W1 = np.random.randn(4,5)\n","b1 = np.random.randn(4,1)\n","W2 = np.random.randn(3,4)\n","b2 = np.random.randn(3,1)\n","W3 = np.random.randn(1,3)\n","b3 = np.random.randn(1,1)\n","\n","t_parameters = {\"W1\": W1,\n","                \"b1\": b1,\n","                \"W2\": W2,\n","                \"b2\": b2,\n","                \"W3\": W3,\n","                \"b3\": b3}\n","\n","t_AL, t_caches = L_model_forward(t_X, t_parameters)\n","\n","print(\"AL = \" + str(t_AL))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLB7HmnmD1Ox","executionInfo":{"status":"ok","timestamp":1719800265633,"user_tz":-480,"elapsed":356,"user":{"displayName":"王震","userId":"15815926928603043085"}},"outputId":"293069b9-91dc-437f-9dde-a0e40486651a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n"]}]},{"cell_type":"markdown","source":["<a name='5'></a>\n","## 5 - Cost Function"],"metadata":{"id":"HsLKP8s0Hh8M"}},{"cell_type":"code","source":["# GRADED FUNCTION: compute_cost\n","\n","def compute_cost(AL, Y):\n","    \"\"\"\n","    Implement the cost function defined by equation (7).\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n","    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n","\n","    Returns:\n","    cost -- cross-entropy cost\n","    \"\"\"\n","\n","    m = Y.shape[1]\n","\n","    # Compute loss from aL and y.\n","\n","    cost = -(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)).sum() / m\n","\n","    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","\n","\n","    return cost"],"metadata":{"id":"IPmBlKMdD2ad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t_Y = np.asarray([[1, 1, 0]])\n","t_AL = np.array([[.8,.9,0.4]])\n","\n","t_cost = compute_cost(t_AL, t_Y)\n","\n","print(\"Cost: \" + str(t_cost))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMeaelCOFcYH","executionInfo":{"status":"ok","timestamp":1719800333879,"user_tz":-480,"elapsed":376,"user":{"displayName":"王震","userId":"15815926928603043085"}},"outputId":"cb698a34-b1d3-458f-b988-109d278dd899"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cost: 0.2797765635793423\n"]}]},{"cell_type":"markdown","source":["<a name='6'></a>\n","## 6 - Backward Propagation Module"],"metadata":{"id":"dZ3Piez8Hl6Q"}},{"cell_type":"markdown","source":["<a name='6-1'></a>\n","### 6.1 - Linear Backward"],"metadata":{"id":"QE0Q2DNsHpAR"}},{"cell_type":"code","source":["# GRADED FUNCTION: linear_backward\n","\n","def linear_backward(dZ, cache):\n","    \"\"\"\n","    Implement the linear portion of backward propagation for a single layer (layer l)\n","\n","    Arguments:\n","    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n","    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n","\n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    dW = np.dot(dZ, A_prev.T) / m\n","    db = dZ.sum(axis=1, keepdims=True) / m\n","    dA_prev = np.dot(W.T, dZ)\n","\n","    return dA_prev, dW, db"],"metadata":{"id":"NlR_FWxdGUhC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def linear_backward_test_case():\n","    \"\"\"\n","    z, linear_cache = (np.array([[-0.8019545 ,  3.85763489]]), (np.array([[-1.02387576,  1.12397796],\n","       [-1.62328545,  0.64667545],\n","       [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), np.array([[1]]))\n","    \"\"\"\n","    np.random.seed(1)\n","    dZ = np.random.randn(3,4)\n","    A = np.random.randn(5,4)\n","    W = np.random.randn(3,5)\n","    b = np.random.randn(3,1)\n","    linear_cache = (A, W, b)\n","    return dZ, linear_cache\n","\n","t_dZ, t_linear_cache = linear_backward_test_case()\n","t_dA_prev, t_dW, t_db = linear_backward(t_dZ, t_linear_cache)\n","\n","print(\"dA_prev: \" + str(t_dA_prev))\n","print(\"dW: \" + str(t_dW))\n","print(\"db: \" + str(t_db))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6voUvADMGVRa","executionInfo":{"status":"ok","timestamp":1719800546793,"user_tz":-480,"elapsed":334,"user":{"displayName":"王震","userId":"15815926928603043085"}},"outputId":"93221af0-9e12-4a4a-f678-4048dfb2ba79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dA_prev: [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n"," [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n"," [-0.4319552  -1.30987417  1.72354705  0.05070578]\n"," [-0.38981415  0.60811244 -1.25938424  1.47191593]\n"," [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n","dW: [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n"," [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n"," [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n","db: [[-0.14713786]\n"," [-0.11313155]\n"," [-0.13209101]]\n"]}]},{"cell_type":"markdown","source":["<a name='6-2'></a>\n","### 6.2 - Linear-Activation Backward"],"metadata":{"id":"w0tfngsUHsqd"}},{"cell_type":"code","source":["# GRADED FUNCTION: linear_activation_backward\n","\n","def linear_activation_backward(dA, cache, activation):\n","    \"\"\"\n","    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n","\n","    Arguments:\n","    dA -- post-activation gradient for current layer l\n","    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    linear_cache, activation_cache = cache\n","\n","    if activation == \"relu\":\n","\n","        dZ = relu_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","\n","    elif activation == \"sigmoid\":\n","\n","        dZ = sigmoid_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","\n","    return dA_prev, dW, db"],"metadata":{"id":"ZTykhTi9GJRw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def linear_activation_backward_test_case():\n","    \"\"\"\n","    aL, linear_activation_cache = (np.array([[ 3.1980455 ,  7.85763489]]), ((np.array([[-1.02387576,  1.12397796], [-1.62328545,  0.64667545], [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), 5), np.array([[ 3.1980455 ,  7.85763489]])))\n","    \"\"\"\n","    np.random.seed(2)\n","    dA = np.random.randn(1,2)\n","    A = np.random.randn(3,2)\n","    W = np.random.randn(1,3)\n","    b = np.random.randn(1,1)\n","    Z = np.random.randn(1,2)\n","    linear_cache = (A, W, b)\n","    activation_cache = Z\n","    linear_activation_cache = (linear_cache, activation_cache)\n","\n","    return dA, linear_activation_cache\n","\n","t_dAL, t_linear_activation_cache = linear_activation_backward_test_case()\n","\n","t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"sigmoid\")\n","print(\"With sigmoid: dA_prev = \" + str(t_dA_prev))\n","print(\"With sigmoid: dW = \" + str(t_dW))\n","print(\"With sigmoid: db = \" + str(t_db))\n","\n","t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"relu\")\n","print(\"With relu: dA_prev = \" + str(t_dA_prev))\n","print(\"With relu: dW = \" + str(t_dW))\n","print(\"With relu: db = \" + str(t_db))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FtletNXMGKIH","executionInfo":{"status":"ok","timestamp":1719800553097,"user_tz":-480,"elapsed":2,"user":{"displayName":"王震","userId":"15815926928603043085"}},"outputId":"d57db235-13bc-46b4-ba19-846d073af43d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["With sigmoid: dA_prev = [[ 0.11017994  0.01105339]\n"," [ 0.09466817  0.00949723]\n"," [-0.05743092 -0.00576154]]\n","With sigmoid: dW = [[ 0.10266786  0.09778551 -0.01968084]]\n","With sigmoid: db = [[-0.05729622]]\n","With relu: dA_prev = [[ 0.44090989  0.        ]\n"," [ 0.37883606  0.        ]\n"," [-0.2298228   0.        ]]\n","With relu: dW = [[ 0.44513824  0.37371418 -0.10478989]]\n","With relu: db = [[-0.20837892]]\n"]}]},{"cell_type":"markdown","source":["<a name='6-3'></a>\n","### 6.3 - L-Model Backward"],"metadata":{"id":"u34Vu-0DHyBw"}},{"cell_type":"code","source":["# GRADED FUNCTION: L_model_backward\n","\n","def L_model_backward(AL, Y, caches):\n","    \"\"\"\n","    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n","\n","    Arguments:\n","    AL -- probability vector, output of the forward propagation (L_model_forward())\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n","                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n","\n","    Returns:\n","    grads -- A dictionary with the gradients\n","             grads[\"dA\" + str(l)] = ...\n","             grads[\"dW\" + str(l)] = ...\n","             grads[\"db\" + str(l)] = ...\n","    \"\"\"\n","    grads = {}\n","    L = len(caches) # the number of layers\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n","\n","    # Initializing the backpropagation\n","\n","    dAL = - (Y / AL - (1 - Y) / (1 - AL)) # cost 函数的导数\n","\n","    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n","\n","    current_cache = caches[L - 1]\n","    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n","    grads[\"dA\" + str(L - 1)] = dA_prev_temp\n","    grads[\"dW\" + str(L)] = dW_temp\n","    grads[\"db\" + str(L)] = db_temp\n","\n","    # Loop from l=L-2 to l=0\n","    for l in reversed(range(L-1)):\n","\n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_prev_temp, current_cache, \"relu\")\n","        grads[\"dA\" + str(l)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","\n","    return grads"],"metadata":{"id":"5Vl7DoOeFoMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def L_model_backward_test_case():\n","    \"\"\"\n","    X = np.random.rand(3,2)\n","    Y = np.array([[1, 1]])\n","    parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747]]), 'b1': np.array([[ 0.]])}\n","\n","    aL, caches = (np.array([[ 0.60298372,  0.87182628]]), [((np.array([[ 0.20445225,  0.87811744],\n","           [ 0.02738759,  0.67046751],\n","           [ 0.4173048 ,  0.55868983]]),\n","    np.array([[ 1.78862847,  0.43650985,  0.09649747]]),\n","    np.array([[ 0.]])),\n","   np.array([[ 0.41791293,  1.91720367]]))])\n","   \"\"\"\n","    np.random.seed(3)\n","    AL = np.random.randn(1, 2)\n","    Y = np.array([[1, 0]])\n","\n","    A1 = np.random.randn(4,2)\n","    W1 = np.random.randn(3,4)\n","    b1 = np.random.randn(3,1)\n","    Z1 = np.random.randn(3,2)\n","    linear_cache_activation_1 = ((A1, W1, b1), Z1)\n","\n","    A2 = np.random.randn(3,2)\n","    W2 = np.random.randn(1,3)\n","    b2 = np.random.randn(1,1)\n","    Z2 = np.random.randn(1,2)\n","    linear_cache_activation_2 = ((A2, W2, b2), Z2)\n","\n","    caches = (linear_cache_activation_1, linear_cache_activation_2)\n","\n","    return AL, Y, caches\n","\n","t_AL, t_Y_assess, t_caches = L_model_backward_test_case()\n","grads = L_model_backward(t_AL, t_Y_assess, t_caches)\n","\n","print(\"dA0 = \" + str(grads['dA0']))\n","print(\"dA1 = \" + str(grads['dA1']))\n","print(\"dW1 = \" + str(grads['dW1']))\n","print(\"dW2 = \" + str(grads['dW2']))\n","print(\"db1 = \" + str(grads['db1']))\n","print(\"db2 = \" + str(grads['db2']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KNzowIidF2Qr","executionInfo":{"status":"ok","timestamp":1719800556185,"user_tz":-480,"elapsed":1,"user":{"displayName":"王震","userId":"15815926928603043085"}},"outputId":"02b73dc3-44a2-4f23-d8cb-1dc129586a94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dA0 = [[ 0.          0.52257901]\n"," [ 0.         -0.3269206 ]\n"," [ 0.         -0.32070404]\n"," [ 0.         -0.74079187]]\n","dA1 = [[ 0.12913162 -0.44014127]\n"," [-0.14175655  0.48317296]\n"," [ 0.01663708 -0.05670698]]\n","dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n"," [0.         0.         0.         0.        ]\n"," [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n","dW2 = [[-0.39202432 -0.13325855 -0.04601089]]\n","db1 = [[-0.22007063]\n"," [ 0.        ]\n"," [-0.02835349]]\n","db2 = [[0.15187861]]\n"]}]},{"cell_type":"markdown","source":["<a name='6-4'></a>\n","### 6.4 - Update Parameters"],"metadata":{"id":"QGIdO2jLH2O1"}},{"cell_type":"code","source":["# GRADED FUNCTION: update_parameters\n","\n","def update_parameters(params, grads, learning_rate):\n","    \"\"\"\n","    Update parameters using gradient descent\n","\n","    Arguments:\n","    params -- python dictionary containing your parameters\n","    grads -- python dictionary containing your gradients, output of L_model_backward\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters\n","                  parameters[\"W\" + str(l)] = ...\n","                  parameters[\"b\" + str(l)] = ...\n","    \"\"\"\n","    parameters = copy.deepcopy(params)\n","    L = len(parameters) // 2 # number of layers in the neural network\n","\n","    # Update rule for each parameter. Use a for loop.\n","    for l in range(L):\n","\n","        parameters[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n","        parameters[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n","\n","    return parameters"],"metadata":{"id":"AvqJBFXSGByC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def update_parameters_test_case():\n","    \"\"\"\n","    parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747],\n","        [-1.8634927 , -0.2773882 , -0.35475898],\n","        [-0.08274148, -0.62700068, -0.04381817],\n","        [-0.47721803, -1.31386475,  0.88462238]]),\n"," 'W2': np.array([[ 0.88131804,  1.70957306,  0.05003364, -0.40467741],\n","        [-0.54535995, -1.54647732,  0.98236743, -1.10106763],\n","        [-1.18504653, -0.2056499 ,  1.48614836,  0.23671627]]),\n"," 'W3': np.array([[-1.02378514, -0.7129932 ,  0.62524497],\n","        [-0.16051336, -0.76883635, -0.23003072]]),\n"," 'b1': np.array([[ 0.],\n","        [ 0.],\n","        [ 0.],\n","        [ 0.]]),\n"," 'b2': np.array([[ 0.],\n","        [ 0.],\n","        [ 0.]]),\n"," 'b3': np.array([[ 0.],\n","        [ 0.]])}\n","    grads = {'dW1': np.array([[ 0.63070583,  0.66482653,  0.18308507],\n","        [ 0.        ,  0.        ,  0.        ],\n","        [ 0.        ,  0.        ,  0.        ],\n","        [ 0.        ,  0.        ,  0.        ]]),\n"," 'dW2': np.array([[ 1.62934255,  0.        ,  0.        ,  0.        ],\n","        [ 0.        ,  0.        ,  0.        ,  0.        ],\n","        [ 0.        ,  0.        ,  0.        ,  0.        ]]),\n"," 'dW3': np.array([[-1.40260776,  0.        ,  0.        ]]),\n"," 'da1': np.array([[ 0.70760786,  0.65063504],\n","        [ 0.17268975,  0.15878569],\n","        [ 0.03817582,  0.03510211]]),\n"," 'da2': np.array([[ 0.39561478,  0.36376198],\n","        [ 0.7674101 ,  0.70562233],\n","        [ 0.0224596 ,  0.02065127],\n","        [-0.18165561, -0.16702967]]),\n"," 'da3': np.array([[ 0.44888991,  0.41274769],\n","        [ 0.31261975,  0.28744927],\n","        [-0.27414557, -0.25207283]]),\n"," 'db1': 0.75937676204411464,\n"," 'db2': 0.86163759922811056,\n"," 'db3': -0.84161956022334572}\n","    \"\"\"\n","    np.random.seed(2)\n","    W1 = np.random.randn(3,4)\n","    b1 = np.random.randn(3,1)\n","    W2 = np.random.randn(1,3)\n","    b2 = np.random.randn(1,1)\n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    np.random.seed(3)\n","    dW1 = np.random.randn(3,4)\n","    db1 = np.random.randn(3,1)\n","    dW2 = np.random.randn(1,3)\n","    db2 = np.random.randn(1,1)\n","    grads = {\"dW1\": dW1,\n","             \"db1\": db1,\n","             \"dW2\": dW2,\n","             \"db2\": db2}\n","\n","    return parameters, grads\n","\n","t_parameters, grads = update_parameters_test_case()\n","t_parameters = update_parameters(t_parameters, grads, 0.1)\n","\n","print (\"W1 = \"+ str(t_parameters[\"W1\"]))\n","print (\"b1 = \"+ str(t_parameters[\"b1\"]))\n","print (\"W2 = \"+ str(t_parameters[\"W2\"]))\n","print (\"b2 = \"+ str(t_parameters[\"b2\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PtQnVa1IGjzm","executionInfo":{"status":"ok","timestamp":1719800603293,"user_tz":-480,"elapsed":350,"user":{"displayName":"王震","userId":"15815926928603043085"}},"outputId":"4d8d00b1-4d1a-4bf0-d456-f67d04d0cfad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n"," [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n"," [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n","b1 = [[-0.04659241]\n"," [-1.28888275]\n"," [ 0.53405496]]\n","W2 = [[-0.55569196  0.0354055   1.32964895]]\n","b2 = [[-0.84610769]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"TGlybC5JGnaD"},"execution_count":null,"outputs":[]}]}